{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47a932-9b5e-4b24-84f1-68813e1a8fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "config = dict()\n",
    "\n",
    "model_config = dict()\n",
    "model_config[\"name\"] = \"DynUNet\"  # network model name from MONAI\n",
    "# set the network hyper-parameters\n",
    "model_config[\"in_channels\"] = 1  #  1 input images for extracting brain region, i.e. DWI\n",
    "model_config[\"out_channels\"] = 1   # brain mask\n",
    "model_config[\"spatial_dims\"] = 3   # 3D input images\n",
    "model_config[\"deep_supervision\"] = False  # do not check outputs of lower layers\n",
    "model_config[\"strides\"] = [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]][:-1]  # number of downsampling convolutions\n",
    "model_config[\"filters\"] = [64, 96, 128, 192, 256, 384, 512, 768, 1024][:len(model_config[\"strides\"])]  # number of filters per layer\n",
    "model_config[\"kernel_size\"] = [[3, 3, 3]] * len(model_config[\"strides\"])  # size of the convolution kernels per layer\n",
    "model_config[\"upsample_kernel_size\"] = model_config[\"strides\"][1:]  # should be the same as the strides\n",
    "\n",
    "# put the model config in the main config\n",
    "config[\"model\"] = model_config\n",
    "\n",
    "config[\"optimizer_one\"] = {'name': 'Adam', \n",
    "                       'lr': 0.0010, # initial learning rate of net 1\n",
    "                      'weight_decay': 0.0005}  \n",
    "\n",
    "config[\"optimizer_two\"] = {'name': 'Adam', \n",
    "                       'lr': 0.0025,  # initial learning rate of net 2\n",
    "                      'weight_decay': 0.0005} \n",
    "\n",
    "# define the loss\n",
    "config[\"loss\"] = {'name': 'DiceLoss', # from Monai\n",
    "                  'include_background': True,  # we do not have a label for the background, so this should be true (by \"include background\" monai means include channel 0)\n",
    "                  'sigmoid': True}  # transform the model logits to activations\n",
    "\n",
    "# writing log files when close the cross validation\n",
    "config[\"training_log_filename\"] = \"training_log.csv\"\n",
    "config[\"model_filename\"] = \"model_final.pth\"\n",
    "\n",
    "\n",
    "# set the cross validation parameters, you can not use cross validation when you write log files\n",
    "# config[\"cross_validation\"] = {'folds': 1,  # number of cross validation folds\n",
    "#                               'seed': 25},  # seed to make the generation of cross validation folds consistent across different trials\n",
    "# set the scheduler parameters\n",
    "config[\"scheduler\"] = {'name': 'ReduceLROnPlateau', \n",
    "                       'patience': 10,  # wait 10 epochs with no improvement before reducing the learning rate\n",
    "                       'factor': 0.5,   # multiply the learning rate by 0.5\n",
    "                       'min_lr': 1e-08}  # stop reducing the learning rate once it gets to 1e-8\n",
    "\n",
    "# set the dataset parameters\n",
    "config[\"dataset\"] = {'name': 'SegmentationDatasetPersistent',  # 'Persistent' means that it will save the preprocessed outputs generated during the first epoch\n",
    "# However, using 'Persistent', does also increase the time of the first epoch compared to the other epochs, which should run faster\n",
    "  'desired_shape': [128, 128, 128],  # resize the images to this shape, increase this to get higher resolution images (increases computation time and memory usage)\n",
    "  'labels': [1],  # 1: brain\n",
    "  'setup_label_hierarchy': False,  # don't changes the labels \n",
    "  'normalization': None,  # already normalized for brain extraction\n",
    "  'resample': True,  # resample the images when resizing them, otherwise the resize could crop out regions of interest\n",
    "  'crop_foreground': True,  # crop the foreground of the images\n",
    "                    }\n",
    "config[\"training\"] = {'batch_size': 1,  # number of image/label pairs to read at a time during training\n",
    "  'validation_batch_size': 1,  # number of image/label pairs to read at atime during validation\n",
    "  'amp': False,  # don't set this to true unless the model you are using is setup to use automatic mixed precision (AMP)\n",
    "  'early_stopping_patience': None,  # stop the model early if the validaiton loss stops improving\n",
    "  'n_epochs': 500,  # number of training epochs, reduce this if you don't want training to run as long\n",
    "  'save_every_n_epochs': 50,  # save the model every n epochs (otherwise only the latest model will be saved)\n",
    "  'save_last_n_models': 10,  # save the last n models \n",
    "  'save_best': True}  # save the model that has the best validation loss\n",
    "\n",
    "# get the training filenames\n",
    "config[\"training_labeled_filenames\"] = list()\n",
    "# Replace \"example_data/baseline_TrainingData/*\" with your own training dataset path\n",
    "for subject_folder in sorted(glob.glob(\"example_data/baseline_TrainingData/*\")):\n",
    "    if not os.path.isdir(subject_folder):\n",
    "        continue\n",
    "    image_filenames = sorted(glob.glob(os.path.join(subject_folder, \"*.nii.gz\")))\n",
    "    for i in range(len(image_filenames)):\n",
    "        if \"brainmask\" in image_filenames[i].lower():  # \"brainmask\" is a string including in my labeled filename, replace \"brainmask\" with the specific string contained in your labeled filenames\n",
    "            label = image_filenames.pop(i)\n",
    "            break\n",
    "    config[\"training_labeled_filenames\"].append({\"image\": image_filenames, \"label\": label})\n",
    "\n",
    "# the unlabeld dataset file path for semi-supervised learning, Replace \"example_data/pseduo_UnlabeledData/*\" with your own unlabeled dataset path\n",
    "# do not need pseudo labels in step 2\n",
    "config[\"training_unlabeled_filenames\"] = list()\n",
    "for subject_folder in sorted(glob.glob(\"example_data/pseduo_UnlabeledData/*\")):\n",
    "    if not os.path.isdir(subject_folder):\n",
    "        continue\n",
    "    image_filenames = sorted(glob.glob(os.path.join(subject_folder, \"*.nii.gz\")))\n",
    "    for i in range(len(image_filenames)):\n",
    "        if \"pseudo_label\" in image_filenames[i].lower():   # \"pseudo_label\" is a string including in my pseudo-label filenames, replace \"brainmask\" with the specific string contained in your pseudo-label filenames\n",
    "            label = image_filenames.pop(i)\n",
    "            break\n",
    "    config[\"training_unlabeled_filenames\"].append({\"image\": image_filenames})\n",
    "\n",
    "# Replace \"example_data/baseline_ValidationData/*\" with your own validation dataset path    \n",
    "config[\"validation_filenames\"] = list()  # \"validation_filenames\" for validation\n",
    "for subject_folder in sorted(glob.glob(\"example_data/baseline_ValidationData/*\")):\n",
    "    if not os.path.isdir(subject_folder):\n",
    "        continue\n",
    "    image_filenames = sorted(glob.glob(os.path.join(subject_folder, \"*.nii.gz\")))\n",
    "    for i in range(len(image_filenames)):\n",
    "        if \"brainmask\" in image_filenames[i].lower():  # \"brainmask\" is a string including in my labeled filename, replace \"brainmask\" with the specific string contained in your labeled filenames\n",
    "            label = image_filenames.pop(i)\n",
    "            break\n",
    "    config[\"validation_filenames\"].append({\"image\": image_filenames, \"label\": label})\n",
    "\n",
    "# Replace \"example_data/baseline_TestData/*\" with your own test dataset path   \n",
    "config[\"test_filenames\"] = list()  # \"test_filenames\" for test, no label\n",
    "for subject_folder in sorted(glob.glob(\"example_data/baseline_TestData/*\")):\n",
    "    if not os.path.isdir(subject_folder):\n",
    "        continue\n",
    "    image_filenames = sorted(glob.glob(os.path.join(subject_folder, \"*.nii.gz\")))\n",
    "    # The 'pseudo_label' string is used to identify and skip prediction files from step 3.\n",
    "    # If your files use a different identifier, replace it here. Otherwise, ignore this.\n",
    "    for i in range(len(image_filenames)):\n",
    "        if \"pseudo_label\" in image_filenames[i].lower():   \n",
    "            label = image_filenames.pop(i)\n",
    "            break\n",
    "    config[\"test_filenames\"].append({\"image\": image_filenames})\n",
    "\n",
    "\n",
    "with open(\"./step2_config.json\", \"w\") as op:\n",
    "    json.dump(config, op, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c22643c9-c02d-4209-a09c-15497f2cc99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baseline_TestData/changgeng_strokecase0001_ses0055/changgeng_strokecase0001_ses-0055_dwi_norm.nii.gz']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079fd306-ad8a-403a-9b12-09203efae6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
